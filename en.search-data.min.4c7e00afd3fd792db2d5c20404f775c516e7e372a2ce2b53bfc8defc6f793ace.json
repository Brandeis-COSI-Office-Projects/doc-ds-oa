[{"id":0,"href":"/doc-ds-oa/docs/doc/","title":"Navigate","section":"Docs","content":"The Whole Documentation Navigation #  This document contains three parts:\n How to collect the data Data processing procedures How to contribute to this documentation  Please read the corresponding section in it\u0026rsquo;s entirety when you start the work, or it will cost a lot to correct the work later on. Read the Collect section when you are starting to collect data; read the Processing section when you are starting to process the data; read the Contribution part when you want to contribute to this page.\nNavigate to the corresponding section:\nCollect\nProcessing\nContribute\n"},{"id":1,"href":"/doc-ds-oa/docs/doc/processing/functions/process/","title":"Process","section":"Functions","content":"Process existing/new data #  Process, as the name suggests, is processing and cleaning all the data you have. You need to have the data folder residing in the directory in order to proceed.\nTwo situations #  There are two situations leading you to encounter different things:\n When you just want to process the old data to see what the result looks like; When you have new grads' information to process  Let\u0026rsquo;s start with the easier one: old data.\nJust process the old data #  After typing p, the following things will pop up:\n\u0026gt;\u0026gt; Entering process mode... - We are looking for raw.csv and algorithm.json file - Found! - Run a preprocessing step to make sure the data format is correct \u0026gt;\u0026gt; There are 1045 out of 1430 lines labelled as \u0026quot;First Pass\u0026quot;, \u0026gt;\u0026gt;\tso 1045 lines will be passed to the next round of real processing. - Preprocessing finished! [!Important] Please be aware that we will not check time format. - Make sure all of your job time is formatted as: xyxm or xy - [Hint: xyxm represents x amount of years and x amount of months. E.g., 3y4m] - Begin real processing - Processed! ...... - Whole process finished! - Please view result/Jul-21-2021-926 folder to see the processed results - You can find cache files at cache/Jul-21-2021-926 folder [ -- See you next time! -- ] You can find console log file in log/Jul-21-2021-926.txt  And the following folders will be generated:\ncache/ log/ result/  Result json #  Let\u0026rsquo;s go to the result folder. That\u0026rsquo;s where the final result resides in. There may be many json files if you run the program multiple times. Find the correct one by checking the file name for this run. In my case, it\u0026rsquo;s Jul-21-2021-926.\nYou can just use this json for visualization or first convert it to csv and then do visualization. It has been processed and cleaned and ready for use.\n You can use clean function to clean up the result folder.\n Cache #  In the cache folder, there are a bunch of files that are generated during data processing. Normally, the folder should contain the following files:\nmerged_processed.json merged_sorted_title.json merged_title.json processed.json sorted_title.json title.json  These files are:\n processed.json the json file after initial extracting from csv; the version without processing and cleaning title.json all the job titles with their occurrences, without processing and cleaning sorted_title.json all the job titles sorted by their occurrences, without processing and cleaning merged_processed.json the dictionary data file after merging job titles with the algorithm merged_title.json all the job titles with their occurrences, after merging merged_sorted_title all the job titles sorted by their occurrences, after merging   You can use clean function to clean up the cache folder, as they are not quite important for visualization.\n Log #  log/ contains all the output in console in case you forget about anything and you have closed the terminal.\n You can use clean function to clean up the log folder, as they are not quite important for visualization, only for your own purpose.\n Dealing with new data #  If you have a bunch of new grads' information to process, you will encounter the \u0026ldquo;deal with new data\u0026rdquo; situation. Your console output might look like this:\n\u0026gt;\u0026gt; Entering process mode... - We are looking for raw.csv and algorithm.json file - Found! - Run a preprocessing step to make sure the data format is correct \u0026gt;\u0026gt; There are 1046 out of 1431 lines labelled as \u0026quot;First Pass\u0026quot;, \u0026gt;\u0026gt;\tso 1046 lines will be passed to the next round of real processing. [!Warning] We have seen some job titles that have not been seen/found/added to our algorithm. We have put all unseen job titles in pending/unseen_titles.txt 1. Open pending/unseen_titles.txt and data/algorithm.json 2. Determine which category(key) does each unseen job title in unseen_titles.txt belong to in algorithm.json 3. Copy each unseen job title to the value array of corresponding category(key) in algorithm.json 4. Rerun the program [ -- See you next time! -- ] You can find console log file in log/Jul-21-2021-912.txt  Most likely there will be some new, unseen job titles that are not included in the algorithm, like \u0026ldquo;professional dog walker\u0026rdquo; or \u0026ldquo;Java and Python and C++ developer\u0026rdquo;. You should follow these steps to refine the algorithm:\n Open pending/unseen_titles.txt file. It has a list of unseen job titles that you will add to the algorithm. Open data/algorithm.json. The keys are job titles to stay because they are the most descriptive and have the most occurrences. The values are job titles to be merged because they are too subtle and only occur once or twice. For example, key-value pair like:\n\u0026#34;Software Engineer\u0026#34;: [ \u0026#34;Java Developer\u0026#34;, \u0026#34;Software Architect\u0026#34; ...... ] Job titles like Java Developer and Software Architect will be replaced by Software Engineer because Software Engineer is more dominant in terms of occurrences and can describe what these two titles basically do. For a more detailed explanation please visit The Documentation for Staff.\n  Determine which category(key) does each unseen job titles fall into. E.g., \u0026ldquo;Linux Engineer\u0026rdquo; falls into \u0026ldquo;Software Engineer\u0026rdquo; and \u0026ldquo;Marketing Research\u0026rdquo; falls into \u0026ldquo;Marketing\u0026rdquo;. Paste the unseen job title inside the value array of the corresponding category(key). E.g., if previously the algorithm.json looks like this:\n\u0026#34;Software Engineer\u0026#34;: [ \u0026#34;Software Developer\u0026#34;, \u0026#34;Developer\u0026#34;, ...... ] ...... And you determine that Coder also fall into this category. After pasting, it should look like this:\n\u0026#34;Software Engineer\u0026#34;: [ \u0026#34;Software Developer\u0026#34;, \u0026#34;Developer\u0026#34;, \u0026#34;Coder\u0026#34;, ...... ] ......    Save  After you finish with merging all the job titles, run process command again and watch what happens.\n"},{"id":2,"href":"/doc-ds-oa/docs/doc/collect/","title":"Collect","section":"Navigate","content":"Steps to collect the data #  File format #  In the beginning of the project, you might be given a Google Sheet or a CSV file to start. If it\u0026rsquo;s a Google Sheet, just stick to it. If it\u0026rsquo;s a CSV file, uploading it to Google Sheet and edit online for easier sharing and collaborating.\nExample of the file #  Whether it\u0026rsquo;s a Google Sheet or CSV, it might look like this:\n   x FIRST LAST \u0026hellip; Initials      1 personAFirstName personALastName \u0026hellip; TZ First Pass   2 personBFirstName personBLastName \u0026hellip; CH Not Found   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;    In the table, some columns looks like this:\n   Job1 Job 1 years of service Job2 Job 2 years of service \u0026hellip; Job 10 Job 10 years of service     Senior Principal Engineer 7y5m Product Marketing 12y \u0026hellip; Senior Software Engineer 3y10m   VP Software Development 2y7m VP System Development 2y \u0026hellip;      These are the columns that you should care about.\nHow to collect the data #  You will presumably be asked to collect the data from LinkedIn. There are two things you should notice on:\n When you are copying job titles, make sure it\u0026rsquo;s copied to the correct cell. E.g., job titles like \u0026ldquo;Software Engineer\u0026rdquo; should not reside in column \u0026ldquo;Job 2 years of service\u0026rdquo;. When you are copying job duration/years of service, make sure to reformat the time to \u0026quot;xyxm\u0026quot; format. \u0026quot;xyxm\u0026quot; represents x amount of years and x amount of months. E.g., 3 years 4 months will be represented as \u0026ldquo;3y4m\u0026rdquo; and 12 years will be represented as \u0026ldquo;12y\u0026rdquo;  For example, none of the following time format is accepted:\n   Job1 Job 1 years of service Job2 Job 2 years of service \u0026hellip; Job 10 Job 10 years of service     Senior Principal Engineer 7 years 5 months Product Marketing 12 yrs \u0026hellip; Senior Software Engineer 3 yr 10 mos   VP Software Development 2 y 7 m VP System Development 2 years \u0026hellip;      Keeping the time format exactly as directed is very important! Data processing and cleaning procedure will not check time format and it uses exactly \u0026quot;xyxm\u0026quot; format to proceed.  How to download the data for processing #   If you are in Google Sheet, download it as a csv file, rename it as raw.csv. If you are in a csv file, rename it as raw.csv  Then, upload \u0026amp; replace the raw.csv in the raw data repository.\nLastly, download the data folder in the raw data repository as directed and put the data folder inside your cloned alumni-analytics directory.\n"},{"id":3,"href":"/doc-ds-oa/docs/doc/processing/quickstart/","title":"Quickstart","section":"Processing","content":"Quickstart #  Prerequisites #   Python 3.9 or higher (I think version \u0026lt; 3.9 should work fine, but I use 3.9, so it\u0026rsquo;s advisable to keep the same Python version to avoid weird problems.) Git Github/Github Desktop Any text editor or IDE. I suggest using Visual Studio Code  Python Version #  python3 --version If you do not have python installed or the version is incorrect, follow the python official documentation to install/upgrade.\nGit Version #  git --version If you do not have git installed, please follow the git official documentation to install Git. If you are using Windows, try to use Git Bash.\nGithub Desktop #  If you are fine using command line to control Github, great! If you are not comfortable using command line, please go ahead and download Github Desktop.\nRepository #  The repository should reside in the Brandeis-COSI-Office-Projects organization. If you don\u0026rsquo;t have access to it, please contact the Department Staff.\nThe repository name is: alumni-analytics\nDownload #   Please clone the repository to your local computer using either command line or Github Desktop. Please download the data folder from the organization. It should reside in this repository. Put the data folder inside the project directory.  Right now, the file directory should look exactly like:\n. ├── README.md ├── code │ ├── clean.py │ ├── console_controller.py │ ├── main.py │ ├── preprocessing.py │ ├── process.py │ └── title_merge.py └── data ├── algorithm.json └── raw.csv  Do a Quick Run #  Run\npython3 code/main.py main.py\nRun main.py will do the data extraction, cleaning, and processing. You can see the whole process displayed in the terminal. Final results will be generated in the data folder.  You have to include code/ in your command, or the code won\u0026rsquo;t work.\nRun exactly as python3 code/main.py\n Check the results #  After the initial run, if everything works well, the file directory should look like this:\n. ├── README.md ├── cache │ └── Jul-21-2021-960 │ ├── merged_processed.json │ ├── merged_sorted_title.json │ ├── merged_title.json │ ├── processed.json │ ├── sorted_title.json │ └── title.json ├── code │ ├── __pycache__ │ │ ├── clean.cpython-39.pyc │ │ ├── console_controller.cpython-39.pyc │ │ ├── handle_new.cpython-39.pyc │ │ ├── message_controller.cpython-39.pyc │ │ ├── preprocessing.cpython-39.pyc │ │ ├── process.cpython-39.pyc │ │ └── title_merge.cpython-39.pyc │ ├── clean.py │ ├── console_controller.py │ ├── main.py │ ├── preprocessing.py │ ├── process.py │ └── title_merge.py ├── data │ ├── algorithm.json │ └── raw.csv ├── log │ └── Jul-21-2021-960.txt └── result └── Jul-21-2021-960.json  It\u0026rsquo;s okay if your file name isn\u0026rsquo;t Jul-21-2021-960. 960 is a randomly-generated three-digit number and Jul-21-2021 is the current date.  Open result/Jul-21-2021-960.json or whatever your file name is.\nThis json file is the final result. You can just use it to do visualization or convert it to csv file.\nNow, let\u0026rsquo;s dig deeper into what\u0026rsquo;s behind the scene.\n"},{"id":4,"href":"/doc-ds-oa/docs/doc/processing/functions/","title":"Functions","section":"Processing","content":"Functions offered by the program #  Three options #  There are three things you can do with the program. The instruction will also pop up when you run python3 code/main.py.\n Process Clean Exit  There are three options you can select: - (p)rocess raw.csv data: use default raw.csv to run data cleaning job - (c)lean cache and trash: clean all the useless data generated before - (e)xit: you can also exit  If you have a handful of new grads' information to process, or if you just want to get the cleaned version of old data, you can proceed with process by just typing p when you are asked to select. When the cache and trash are getting overwhelming, you can proceed with clean by typing c. If you mistakenly run the program, you can just proceed with exit by typing e.  "},{"id":5,"href":"/doc-ds-oa/docs/doc/processing/functions/clean/","title":"Clean","section":"Functions","content":"Clean cache and trash #   Please be aware that \u0026ldquo;Clean\u0026rdquo; here does not refer to data cleaning, but refer to cleaning cache and trash generated during the data processing and cleaning job.\n If you run the clean function, your console might look like this:\n\u0026gt;\u0026gt; Entering clean cache mode... We have removed unnecessary folders. [ -- See you next time! -- ] You can find console log file in log/Jul-21-2021-158.txt  And you should notice that your cache, log, result, and pending folder have been removed (if you have one).\n"},{"id":6,"href":"/doc-ds-oa/docs/doc/processing/functions/exit/","title":"Exit","section":"Functions","content":"Exit the program #  You can simply run the exit function to exit the program.\n Please note that exit will still generate console logs inside the log folder.\n "},{"id":7,"href":"/doc-ds-oa/docs/doc/processing/code-structure/","title":"Code Structure","section":"Processing","content":"The code structure #  Although the code are well-documented and structured, it is still worth explaining the overall structure for you to familiarize.\ncode directory #  The code directory looks like this:\n. ├── clean.py ├── console_controller.py ├── main.py ├── preprocessing.py ├── process.py └── title_merge.py   clean.py It will merge job titles based on the algorithm and unify time format to be xx.xx years. console_controller.py It controls all the console output \u0026amp; recording. main.py The execution program that will run processing and cleaning. preprocessing.py Run the preprocessing step to make sure the data format is correct (only checks job titles). process.py Extract the json information from csv file for cleaning. title_merge.py Algorithm of merging job titles.  data directory #  Additionally, the data folder looks like this:\n. ├── algorithm.json └── raw.csv   algorithm.json It contains the algorithm to merge job titles. raw.csv It contains the raw alumni data.  "},{"id":8,"href":"/doc-ds-oa/docs/doc/contribute/","title":"Contribute","section":"Navigate","content":"How to contribute to this documentation #  Hugo #  This documentation is built on Hugo. Hugo is a really fast static site generator built by Go. You will need prior Hugo and Markdown knowledge to proceed.\nHugo Documentation #  The Hugo Documentation is pretty useful for learning hugo. You don\u0026rsquo;t need to know all the details. Just basic hugo commands, file directory, and how to write new pages.\nI also recommend watching Mike Danne\u0026rsquo;s Youtube video.\nThis project uses Hugo Book theme to render the documentation.\nAfter learning a bit of Hugo, you can start to contribute to this documentation. Simply git clone the repository to your local computer and start writing/modifying. Do not touch the .github directory!\nMarkdown #  All the files are written in Markdown. I suggest taking look at the Markdown Cheatsheet if you don\u0026rsquo;t have prior Markdown knowledge.\n"}]